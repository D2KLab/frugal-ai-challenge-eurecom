{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3114dcac-9f1c-49bd-80a3-1d28349b8d19",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfafa51-0ad9-497e-bb05-7e873012c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import sklearn.metrics as metrics\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForPreTraining, BertModel, AutoTokenizer, AutoModel, ModernBertForSequenceClassification, BertForSequenceClassification, RobertaForSequenceClassification\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "from torch.optim import AdamW\n",
    "import copy\n",
    "\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d1ee3",
   "metadata": {},
   "source": [
    "# Parameteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a797c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVAILABLE MODELS\n",
    "# sk-rf: Sbert + sklearn SVM\n",
    "# mlp: Sbert + MLP\n",
    "# ct: Covid-twitter-BERT\n",
    "# modern-base: Modern-BERT-base\n",
    "# modern-large: Modern-BERT-large\n",
    "# gte-base: gte-base-en-v1.5\n",
    "# gte-large: gte-large-en-v1.5\n",
    "\n",
    "MODEL = \"modern-base\"\n",
    "FINAL_SUBMISSION = False # Train on full data if True, Tarin on 80% if False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7e386-958b-4c7b-b60a-a7d883d77d28",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036f470-90f2-4532-b01e-7b0e152251c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"quotaclimat/frugalaichallenge-text-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd20ed-ff75-4e44-9877-45d07878bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\n",
    "    '0_not_relevant',\n",
    "    '1_not_happening'\n",
    "    '2_not_human',\n",
    "    '3_not_bad',\n",
    "    '4_solutions_harmful_unnecessary',\n",
    "    '5_science_unreliable',\n",
    "    '6_proponents_biased',\n",
    "    '7_fossil_fuels_needed'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ca1b5-badc-4465-a329-bcf1604e4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = dataset['train']\n",
    "data_test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94ee54-aacb-4c95-8e93-03e7d8b8f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [t['quote'] for t in data_train]\n",
    "test_texts = [t['quote'] for t in data_test]\n",
    "\n",
    "\n",
    "\n",
    "labels_train = [int(t['label'][0]) for t in data_train]\n",
    "labels_test = [int(t['label'][0]) for t in data_test]\n",
    "\n",
    "if FINAL_SUBMISSION:\n",
    "    train_texts = train_texts+test_texts\n",
    "    labels_train = labels_train+labels_test\n",
    "\n",
    "\n",
    "weights_tmp = []\n",
    "for i in range(0, 8):\n",
    "    weights_tmp.append(labels_train.count(i))\n",
    "\n",
    "weights = [len(labels_train)/(w+1) for w in weights_tmp]\n",
    "\n",
    "weights = torch.FloatTensor(weights).to(device)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20067062-c941-4d49-adc0-e560074e18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL in ['sk-rf']:\n",
    "    emb_model = SentenceTransformer(\"sentence-transformers/sentence-t5-large\")\n",
    "    batch_size = 512\n",
    "\n",
    "    train_texts = torch.Tensor(emb_model.encode(train_texts))\n",
    "    train_labels = labels_train\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    train_data = TensorDataset(train_texts, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    test_texts = torch.Tensor(emb_model.encode(test_texts))\n",
    "    test_labels = labels_test\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    test_data = TensorDataset(test_texts, test_labels)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "if MODEL in ['mlp']:\n",
    "    emb_model = SentenceTransformer(\"sentence-transformers/sentence-t5-large\")\n",
    "    batch_size = 2\n",
    "\n",
    "    train_texts = torch.Tensor(emb_model.encode(train_texts))\n",
    "    train_labels = labels_train\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    train_data = TensorDataset(train_texts, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    test_texts = torch.Tensor(emb_model.encode(test_texts))\n",
    "    test_labels = labels_test\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    test_data = TensorDataset(test_texts, test_labels)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "elif MODEL==\"ct\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
    "    MAX_LEN = 256\n",
    "    \n",
    "    tokenized_input = tokenizer(train_texts, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "    tokenized_test = tokenizer(test_texts, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "    \n",
    "    train_input_ids, train_token_type_ids, train_attention_mask = tokenized_input['input_ids'], tokenized_input['token_type_ids'], tokenized_input['attention_mask']\n",
    "    test_input_ids, test_token_type_ids, test_attention_mask = tokenized_test['input_ids'], tokenized_test['token_type_ids'], tokenized_test['attention_mask']\n",
    "    \n",
    "    train_token_type_ids = torch.tensor(train_token_type_ids)\n",
    "    test_token_type_ids = torch.tensor(test_token_type_ids)\n",
    "    \n",
    "    \n",
    "    train_labels = labels_train\n",
    "    test_labels = labels_test\n",
    "    \n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    train_input_ids = torch.tensor(train_input_ids)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    train_attention_mask = torch.tensor(train_attention_mask)\n",
    "    \n",
    "    test_input_ids = torch.tensor(test_input_ids)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    test_attention_mask = torch.tensor(test_attention_mask)\n",
    "    batch_size = 16\n",
    "\n",
    "    train_data = TensorDataset(train_input_ids, train_attention_mask, train_token_type_ids, train_labels)\n",
    "    test_data = TensorDataset(test_input_ids, test_attention_mask, test_token_type_ids, test_labels)\n",
    "    \n",
    "    \n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "    \n",
    "elif MODEL==\"gte-large\" or MODEL==\"gte-base\":\n",
    "    MAX_LEN = 256\n",
    "    tokenizer = tokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/'+MODEL+'-en-v1.5')\n",
    "    tokenized_input = tokenizer(train_texts, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "    tokenized_test = tokenizer(test_texts, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "    \n",
    "    train_input_ids, train_attention_mask, train_token_type_ids = tokenized_input['input_ids'], tokenized_input['attention_mask'], tokenized_input['token_type_ids']\n",
    "    test_input_ids, test_attention_mask, test_token_type_ids = tokenized_test['input_ids'], tokenized_test['attention_mask'], tokenized_test['token_type_ids']\n",
    "    \n",
    "    train_labels = labels_train\n",
    "    test_labels = labels_test\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    train_input_ids = torch.tensor(train_input_ids)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    train_attention_mask = torch.tensor(train_attention_mask)\n",
    "    train_token_type_ids = torch.tensor(train_token_type_ids)\n",
    "    \n",
    "    test_input_ids = torch.tensor(test_input_ids)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    test_attention_mask = torch.tensor(test_attention_mask)\n",
    "    test_token_type_ids = torch.tensor(test_token_type_ids)\n",
    "\n",
    "    batch_size = 16\n",
    "\n",
    "    train_data = TensorDataset(train_input_ids, train_attention_mask, train_token_type_ids, train_labels)\n",
    "    test_data = TensorDataset(test_input_ids, test_attention_mask, test_token_type_ids, test_labels)\n",
    "    \n",
    "    \n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "elif MODEL == \"modern-large\" or MODEL==\"modern-base\":\n",
    "    if MODEL == \"modern-large\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-large\")\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "        \n",
    "    MAX_LEN = 256\n",
    "\n",
    "    tokenized_input = tokenizer(train_texts, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "    tokenized_test = tokenizer(test_texts, max_length=MAX_LEN, padding='max_length', truncation=True)\n",
    "    \n",
    "    train_input_ids, train_attention_mask = tokenized_input['input_ids'], tokenized_input['attention_mask']\n",
    "    test_input_ids, test_attention_mask = tokenized_test['input_ids'], tokenized_test['attention_mask']    \n",
    "    \n",
    "    train_labels = labels_train\n",
    "    test_labels = labels_test\n",
    "    \n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    train_input_ids = torch.tensor(train_input_ids)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    train_attention_mask = torch.tensor(train_attention_mask)\n",
    "    \n",
    "    test_input_ids = torch.tensor(test_input_ids)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    test_attention_mask = torch.tensor(test_attention_mask)\n",
    "\n",
    "    batch_size = 16\n",
    "    train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "    test_data = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
    "    \n",
    "    \n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557e55e4-81fb-4b72-999f-fd08bfcc3580",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb2692-ca5a-40ab-91f0-2b1c570ce17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTBERT(\n",
    "    nn.Module,\n",
    "    PyTorchModelHubMixin, \n",
    "    # optionally, you can add metadata which gets pushed to the model card\n",
    "):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = BertForPreTraining.from_pretrained('digitalepidemiologylab/covid-twitter-bert-v2')    \n",
    "        self.bert.cls.seq_relationship = nn.Linear(1024, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = input_mask)\n",
    "        logits = outputs[1]\n",
    "        \n",
    "        return logits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422bc0ce-81da-45ed-a1c7-4de288867350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conspiracyModelLarge(\n",
    "    nn.Module,\n",
    "    PyTorchModelHubMixin, \n",
    "    # optionally, you can add metadata which gets pushed to the model card\n",
    "):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = num_classes\n",
    "        self.bert = ModernBertForSequenceClassification.from_pretrained('answerdotai/ModernBERT-large', num_labels=num_classes)    \n",
    "        \n",
    "    def forward(self, input_ids, input_mask):\n",
    "        outputs = self.bert(input_ids = input_ids, attention_mask = input_mask)\n",
    "        \n",
    "        return outputs.logits\n",
    "\n",
    "class conspiracyModelBase(\n",
    "    nn.Module,\n",
    "    PyTorchModelHubMixin, \n",
    "    # optionally, you can add metadata which gets pushed to the model card\n",
    "):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = num_classes\n",
    "        self.bert = ModernBertForSequenceClassification.from_pretrained('answerdotai/ModernBERT-base', num_labels=num_classes)    \n",
    "        \n",
    "    def forward(self, input_ids, input_mask):\n",
    "        outputs = self.bert(input_ids = input_ids, attention_mask = input_mask)\n",
    "        \n",
    "        return outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa3ceb-6b64-4cda-988a-146fdee7d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gteModelLarge(\n",
    "    nn.Module,\n",
    "    PyTorchModelHubMixin, \n",
    "    # optionally, you can add metadata which gets pushed to the model card\n",
    "):    \n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = num_classes\n",
    "        self.gte = AutoModel.from_pretrained('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)\n",
    "        self.cls = nn.Linear(1024, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, input_type_ids):\n",
    "        outputs = self.gte(input_ids = input_ids, attention_mask = input_mask, token_type_ids = input_type_ids)\n",
    "        embeddings = outputs.last_hidden_state[:, 0]\n",
    "        logits = self.cls(embeddings)\n",
    "        return logits\n",
    "\n",
    "class gteModelBase(\n",
    "    nn.Module,\n",
    "    PyTorchModelHubMixin, \n",
    "    # optionally, you can add metadata which gets pushed to the model card\n",
    "):    \n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = num_classes\n",
    "        self.gte = AutoModel.from_pretrained('Alibaba-NLP/gte-base-en-v1.5', trust_remote_code=True)\n",
    "        self.cls = nn.Linear(768, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, input_type_ids):\n",
    "        outputs = self.gte(input_ids = input_ids, attention_mask = input_mask, token_type_ids = input_type_ids)\n",
    "        embeddings = outputs.last_hidden_state[:, 0]\n",
    "        logits = self.cls(embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13d6a4-96e2-41b9-a260-3aca6dd7fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConspiracyClassification(\n",
    "    nn.Module,\n",
    "    PyTorchModelHubMixin, \n",
    "    # optionally, you can add metadata which gets pushed to the model card\n",
    "):    \n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(768, 100)\n",
    "        self.h2 = nn.Linear(100, 100)\n",
    "        self.h3 = nn.Linear(100, 100)\n",
    "        self.h4 = nn.Linear(100, 50)\n",
    "        self.h5 = nn.Linear(50, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, input_texts):\n",
    "        outputs = self.h1(input_texts)\n",
    "        outputs = self.activation(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.h2(outputs)\n",
    "        outputs = self.activation(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.h3(outputs)\n",
    "        outputs = self.activation(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.h4(outputs)\n",
    "        outputs = self.activation(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.h5(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01a744-115d-47bc-a12f-103512c152dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = {\"num_classes\": 8}\n",
    "if MODEL in ['sk-rf']:\n",
    "    model = SGDClassifier(loss ='hinge', \n",
    "                          alpha=0.0001)\n",
    "    model = SVC()\n",
    "    \n",
    "else:\n",
    "    if MODEL ==\"ct\":\n",
    "        model = CTBERT(**config)\n",
    "    elif MODEL ==\"gte-base\":\n",
    "        model = gteModelBase(**config)\n",
    "    elif MODEL ==\"gte-large\":\n",
    "        model = gteModelLarge(**config)\n",
    "    elif MODEL ==\"modern-base\":\n",
    "        model = conspiracyModelBase(**config)\n",
    "    elif MODEL ==\"modern-large\":\n",
    "        model = conspiracyModelLarge(**config)\n",
    "    elif MODEL ==\"mlp\":\n",
    "        model = ConspiracyClassification(**config)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da29a4-20ad-4f36-b83d-d82d30328f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL not in ['sk-rf']:\n",
    "    if MODEL == \"mlp\":\n",
    "        lr = 5e-4\n",
    "    else:\n",
    "        lr = 2e-5\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr=lr,\n",
    "                      weight_decay = 0.01)\n",
    "    \n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=4, factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e71c8-503d-4a80-8338-24d89e670e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight = weights)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e04f4",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6620b15e-f9e2-4f31-a7ec-6daebf47722d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "\n",
    "best_MCCA = 0\n",
    "best_F1 = 0\n",
    "best_loss = 999\n",
    "best_ACC = 0\n",
    "results = []\n",
    "\n",
    "if MODEL not in ['sk-rf']:\n",
    "    best_state_dict = model.state_dict()\n",
    "else:\n",
    "    best_state_dict = None\n",
    "best_epoch = 0\n",
    "\n",
    "for e in trange(0, epochs, position=0, leave=True):\n",
    "\n",
    "    print('Starting epoch ', e)\n",
    "    if MODEL not in ['sk-rf']:\n",
    "        model.train()\n",
    "        \n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    x_features = []\n",
    "    y_true = []\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if MODEL in ['sk-rf']:\n",
    "            x, y = batch\n",
    "            x_features.extend(x)\n",
    "            y_true.extend(y)\n",
    "        else:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            if MODEL in ['ct', 'gte-base', 'gte-large']:\n",
    "                b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
    "            elif MODEL in ['modern-base', 'modern-large']:\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "            elif MODEL in ['mlp']:\n",
    "                b_input_ids, b_labels = batch\n",
    "            \n",
    "            b_labels = b_labels.float()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            if MODEL in ['ct', 'gte-base', 'gte-large']:\n",
    "                logits = model(b_input_ids, b_input_mask, b_token_type_ids)\n",
    "            elif MODEL in ['modern-base', 'modern-large']:\n",
    "                logits = model(b_input_ids, b_input_mask)\n",
    "            elif MODEL in ['mlp']:\n",
    "                logits = model(b_input_ids)\n",
    "        \n",
    "                \n",
    "            loss = criterion(logits, b_labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "    \n",
    "    if MODEL in ['sk-rf']:\n",
    "        model.fit(x_features, y_true)\n",
    "    else:\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "    if MODEL not in ['sk-rf']:\n",
    "        model.eval()\n",
    "    \n",
    "    predictions_sep = []\n",
    "    \n",
    "    labels_sep = []\n",
    "    \n",
    "    eval_loss = 0\n",
    "    steps=0\n",
    "    x_features = []\n",
    "    y_true = []\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        if MODEL in ['sk-rf']:\n",
    "            x, y = batch\n",
    "            x_features.extend(x)\n",
    "            y_true.extend(y)\n",
    "        else:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "            if MODEL in ['ct', 'gte-base', 'gte-large']:\n",
    "                b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
    "            elif MODEL in ['modern-base', 'modern-large']:\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "            elif MODEL in ['mlp', 'sk-rf']:\n",
    "                b_input_ids, b_labels = batch\n",
    "                \n",
    "            b_labels = b_labels.float()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "    \n",
    "                if MODEL in ['ct', 'gte-base', 'gte-large']:\n",
    "                    logits = model(b_input_ids, b_input_mask, b_token_type_ids)\n",
    "                elif MODEL in ['modern-base', 'modern-large']:\n",
    "                    logits = model(b_input_ids, b_input_mask)\n",
    "\n",
    "                elif MODEL in ['mlp']:\n",
    "                    logits = model(b_input_ids)\n",
    "\n",
    "                loss = criterion(logits, b_labels.long())\n",
    "        \n",
    "    \n",
    "    \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            ground_truth = b_labels.detach().cpu().numpy()\n",
    "            \n",
    "            steps+=1\n",
    "            eval_loss+=loss.detach().item()\n",
    "            predictions_sep.extend(logits.argmax(1))\n",
    "            for l in ground_truth:\n",
    "                labels_sep.append(l)\n",
    "        \n",
    "    if MODEL in ['sk-rf']:\n",
    "        LOSS = 999\n",
    "        predictions_sep = model.predict(x_features).tolist()\n",
    "        labels_sep = y_true\n",
    "    else:\n",
    "        scheduler.step(eval_loss/steps)\n",
    "        LOSS = eval_loss/steps\n",
    "    \n",
    "    ACC = metrics.accuracy_score(labels_sep, predictions_sep)\n",
    "    F1 = metrics.f1_score(labels_sep, predictions_sep, average='macro')\n",
    "    MCCA = metrics.matthews_corrcoef(labels_sep, predictions_sep)\n",
    "    \n",
    "    if ACC> best_ACC:\n",
    "        best_MCCA = MCCA\n",
    "        best_ACC = ACC\n",
    "        best_F1 = F1\n",
    "        best_loss = LOSS\n",
    "        if MODEL not in ['sk-rf']:\n",
    "            best_state_dict = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            best_state_dict = None\n",
    "        best_epoch = e\n",
    "    results.append([LOSS, ACC, F1, MCCA])\n",
    "    print(\"\\t Eval loss: {}\".format(LOSS))\n",
    "    print(\"\\t Eval ACC: {}\".format(ACC))\n",
    "    print(\"\\t Eval F1: {}\".format(F1))\n",
    "    print(\"\\t Eval MCCA: {}\".format(MCCA))\n",
    "    print(\"---\"*25)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498edf9-dc6c-4701-9f2f-6794599ea42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL != \"sk-rf\":\n",
    "    model.load_state_dict(best_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec3430-8f23-402f-94cd-c157caa2845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "HF_token = \"<YOUR_TOKEN>\"\n",
    "login(HF_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ed9b5-704d-4c47-b123-f4336b7e8b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./trained_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
